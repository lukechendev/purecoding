{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6ebcebe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7dbae5b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bag of Words Meets Bags of Popcorn\\nhttps://www.kaggle.com/competitions/word2vec-nlp-tutorial\\n    \\nData fields\\n    id - Unique ID of each review\\n    sentiment - Sentiment of the review; 1 for positive reviews and 0 for negative reviews\\n    review - Text of the review\\n'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Bag of Words Meets Bags of Popcorn\n",
    "https://www.kaggle.com/competitions/word2vec-nlp-tutorial\n",
    "    \n",
    "Data fields\n",
    "    id - Unique ID of each review\n",
    "    sentiment - Sentiment of the review; 1 for positive reviews and 0 for negative reviews\n",
    "    review - Text of the review\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3e65e848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= train set:  (25000, 3)\n",
      "            id  sentiment                                             review\n",
      "0       5814_8          1  With all this stuff going down at the moment w...\n",
      "1       2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
      "2       7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
      "3       3630_4          0  It must be assumed that those who praised this...\n",
      "4       9495_8          1  Superbly trashy and wondrously unpretentious 8...\n",
      "...        ...        ...                                                ...\n",
      "24995   3453_3          0  It seems like more consideration has gone into...\n",
      "24996   5064_1          0  I don't believe they made this film. Completel...\n",
      "24997  10905_3          0  Guy is a loser. Can't get girls, needs to buil...\n",
      "24998  10194_3          0  This 30 minute documentary Bu√±uel made in the ...\n",
      "24999   8478_8          1  I saw this movie as a child and it broke my he...\n",
      "\n",
      "[25000 rows x 3 columns] \n",
      "\n",
      "========= test set:  (25000, 2)\n",
      "             id                                             review\n",
      "0      12311_10  Naturally in a film who's main themes are of m...\n",
      "1        8348_2  This movie is a disaster within a disaster fil...\n",
      "2        5828_4  All in all, this is a movie for kids. We saw i...\n",
      "3        7186_2  Afraid of the Dark left me with the impression...\n",
      "4       12128_7  A very accurate depiction of small time mob li...\n",
      "...         ...                                                ...\n",
      "24995   2155_10  Sony Pictures Classics, I'm looking at you! So...\n",
      "24996     59_10  I always felt that Ms. Merkerson had never got...\n",
      "24997    2531_1  I was so disappointed in this movie. I am very...\n",
      "24998    7772_8  From the opening sequence, filled with black a...\n",
      "24999  11465_10  This is a great horror film for people who don...\n",
      "\n",
      "[25000 rows x 2 columns] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train = pd.read_csv('data/labeledTrainData.tsv',delimiter='\\t')\n",
    "test = pd.read_csv('data/testData.tsv',delimiter='\\t')\n",
    "\n",
    "print('========= train set: ', train.shape)\n",
    "print(train, '\\n')\n",
    "print('========= test set: ', test.shape)\n",
    "print(test, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7cbd77cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "# data preprocessing\n",
    "import re\n",
    "\n",
    "def review_preprocessing(review):\n",
    "    # Only alphabet\n",
    "    review_filtered = re.sub(\"[^a-zA-Z]\", \" \", review)\n",
    "    \n",
    "    # Only lowercase\n",
    "    review_filtered = review_filtered.lower()\n",
    "    \n",
    "    # Remove stop words\n",
    "    # TBD\n",
    "    \n",
    "    return (review_filtered)\n",
    "\n",
    "full_train_y = train['sentiment']\n",
    "full_train_X = []\n",
    "for review in train['review']:\n",
    "    full_train_X.append(review_preprocessing(review))\n",
    "full_train_X = np.array(full_train_X)\n",
    "\n",
    "full_test_X = []\n",
    "for review in test['review']:\n",
    "    full_test_X.append(review_preprocessing(review))\n",
    "full_test_X = np.array(full_test_X)\n",
    "\n",
    "print(full_train_X.shape)\n",
    "print(full_test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e5b2b8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000,)\n",
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "# data spliting\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_X, validation_X, train_y, validation_y = train_test_split(\n",
    "    full_train_X,\n",
    "    full_train_y,\n",
    "    test_size=0.2,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "print(train_X.shape)\n",
    "print(validation_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ea96f546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: 67009\n"
     ]
    }
   ],
   "source": [
    "# build vocabulary dictionary on train data\n",
    "def GetVocabulary(data):\n",
    "    vocab_dict = {}\n",
    "    wid = 0\n",
    "    for document in data:\n",
    "        words = document.split()\n",
    "        for word in words:\n",
    "            if word not in vocab_dict:\n",
    "                vocab_dict[word] = wid\n",
    "                wid += 1\n",
    "    \n",
    "    return vocab_dict\n",
    "\n",
    "vocab_dict = GetVocabulary(train_X)\n",
    "print('Vocabulary: ' + str(len(vocab_dict.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ca1b28f2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train matrix: 20000 x 67009\n"
     ]
    }
   ],
   "source": [
    "# vectorize document\n",
    "def Document2Vector(vocab_dict, data):\n",
    "    word_vector = np.zeros(len(vocab_dict.keys()))\n",
    "    words = data.split()\n",
    "    out_of_voc = 0\n",
    "    for word in words:\n",
    "        if word in vocab_dict:\n",
    "            word_vector[vocab_dict[word]] += 1\n",
    "        else:\n",
    "            out_of_voc += 1\n",
    "            \n",
    "        return word_vector, out_of_voc\n",
    "\n",
    "train_matrix = []\n",
    "for document in train_X:\n",
    "    word_vector, _ = Document2Vector(vocab_dict, document)\n",
    "    train_matrix.append(word_vector)\n",
    "    \n",
    "print('Train matrix: ' + str(len(train_matrix)) + ' x ' + str(len(train_matrix[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "77982d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on doc id:0\n",
      "Train on doc id:2000\n",
      "Train on doc id:4000\n",
      "Train on doc id:6000\n",
      "Train on doc id:8000\n",
      "Train on doc id:10000\n",
      "Train on doc id:12000\n",
      "Train on doc id:14000\n",
      "Train on doc id:16000\n",
      "Train on doc id:18000\n"
     ]
    }
   ],
   "source": [
    "# naive bayes training on two labels\n",
    "def NaiveBayes_train(train_matrix, train_y, label_1, label_2):\n",
    "    num_docs = len(train_matrix)\n",
    "    num_words = len(train_matrix[0])\n",
    "    \n",
    "    # counter initialized with smoothing 1\n",
    "    label_1_word_counter = np.ones(num_words)\n",
    "    label_2_word_counter = np.ones(num_words)\n",
    "    \n",
    "    label_1_total_word_count = 0\n",
    "    label_2_total_word_count = 0\n",
    "    \n",
    "    label_1_count = 0\n",
    "    label_2_count = 0\n",
    "    \n",
    "    for i in range(num_docs):\n",
    "        if i % (num_docs / 10) == 0:\n",
    "            print('Train on doc id:' + str(i))\n",
    "            \n",
    "        if train_y.values[i] == label_1:\n",
    "            label_1_word_counter += train_matrix[i]\n",
    "            label_1_total_word_count += sum(train_matrix[i])\n",
    "            label_1_count += 1\n",
    "        elif train_y.values[i] == label_2:\n",
    "            label_2_word_counter += train_matrix[i]\n",
    "            label_2_total_word_count += sum(train_matrix[i])\n",
    "            label_2_count += 1\n",
    "            \n",
    "        p_label_1_vector = np.log(label_1_word_counter / (label_1_total_word_count + num_words)) # with smoothing\n",
    "        p_label_2_vector = np.log(label_2_word_counter / (label_2_total_word_count + num_words)) # with smoothing\n",
    "    \n",
    "    return p_label_1_vector, np.log(label_1_count/num_docs), p_label_2_vector, np.log(label_2_count/num_docs), label_1_total_word_count, label_2_total_word_count\n",
    "\n",
    "p_neg_vector, p_neg, p_pos_vector, p_pos, neg_total_count, pos_total_count = NaiveBayes_train(train_matrix, train_y, 0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ac7e3fd3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test on the doc id: 0\n",
      "Test on the doc id: 500\n",
      "Test on the doc id: 1000\n",
      "Test on the doc id: 1500\n",
      "Test on the doc id: 2000\n",
      "Test on the doc id: 2500\n",
      "Test on the doc id: 3000\n",
      "Test on the doc id: 3500\n",
      "Test on the doc id: 4000\n",
      "Test on the doc id: 4500\n",
      "Prediction on verification: 5000\n"
     ]
    }
   ],
   "source": [
    "# prediction for validation\n",
    "def Predict(test_word_vector, p_neg_vector, p_neg, p_pos_vector, p_pos, neg_smoothing, pos_smoothing):\n",
    "    neg = sum(test_word_vector * p_neg_vector) + p_neg + neg_smoothing\n",
    "    pos = sum(test_word_vector * p_pos_vector) + p_pos + pos_smoothing\n",
    "    if (neg > pos):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "num_words = len(vocab_dict.keys())\n",
    "predictions = []\n",
    "i = 0\n",
    "for document in validation_X:\n",
    "    if (i % (validation_X.shape[0] / 10) == 0):\n",
    "        print('Test on the doc id: ' + str(i))\n",
    "    i += 1\n",
    "    \n",
    "    test_word_vector, out_of_voc = Document2Vector(vocab_dict, document)\n",
    "    \n",
    "    # smoothing\n",
    "    if (out_of_voc != 0):\n",
    "        neg_smoothing = np.log(out_of_voc / (neg_total_count + num_words))\n",
    "        pos_smoothing = np.log(out_of_voc / (pos_total_count + num_words))\n",
    "    else:\n",
    "        neg_smoothing = 0\n",
    "        pos_smoothing = 0\n",
    "        \n",
    "    ans = Predict(test_word_vector, p_neg_vector, p_neg, p_pos_vector, p_pos, neg_smoothing, pos_smoothing)\n",
    "    predictions.append(ans)\n",
    "    \n",
    "print('Prediction on verification: ' + str(len(predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0496a591",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5512\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.84      0.65      2459\n",
      "           1       0.64      0.27      0.38      2541\n",
      "\n",
      "    accuracy                           0.55      5000\n",
      "   macro avg       0.58      0.56      0.51      5000\n",
      "weighted avg       0.58      0.55      0.51      5000\n",
      "\n",
      "[[2069  390]\n",
      " [1854  687]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "print(accuracy_score(validation_y.values, predictions))\n",
    "print(classification_report(validation_y.values, predictions))\n",
    "print(confusion_matrix(validation_y, predictions))\n",
    "\n",
    "\n",
    "# print(validation_y.values[:100])\n",
    "# print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95595b28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
